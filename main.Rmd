---
title: "Progetto ML"
author: "Manuel Kosovel, Ivan Pelizon"
date: "8/12/2020"
output: html_document
---

Packages needed

```{r}
require(tidyverse)
require(rlang)
```


Useful imported functions
```{r}
# return vector of errors for each fold
# can specify formula, learning technique, data and number of folds (k)
cv.error = function(formula, learner, data, k, ...) {
  indexes = sample(nrow(data))
  errs = c(1:k) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/k*(i-1)+1):(nrow(data)/k*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = learner(formula, data[-indexes.test,], ...)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(formula))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:k))
  errs
}
```

User defined functions
```{r}
oob.error.RF = function(formula, data, ...) {
  model.RF = randomForest(formula, data, ...)
  confusion.matrix.RF = model.RF$confusion[,-ncol(model.RF$confusion)]
  1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)
}
```

First we import the csv
```{r}
data = read.csv("C:\\Users\\aleks\\Documents\\drive\\uni\\Progetti\\ML\\progetti 2019_20\\leaf\\leaf.csv")
#data = read.csv("/home/peiva/Documents/Sync documents/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
#data = read.csv("/home/peiva/Documents/Sync documents/Ivan/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
```

Change column names
```{r}
colnames(data) <- c("Class","SpecimenNumber","Eccentricity","AspectRatio","Elongation",
                    "Solidity","StochasticConvexity","IsoperimetricFactor",
                    "MaximalIndentationDepth","Lobedness","AverageIntensity" , 
                    "AverageContrast","Smoothness","ThirdMoment","Uniformity","Entropy")
# remove specimen count column
# data = data[,-2]
# SpecimenCount value reduces OOB error on RF by 3%, keep it
```

Histogram, available observations count
```{r}
count.byClass = data %>% group_by(Class) %>% count(Class)
colnames(count.byClass) = c("Classe di pianta", "Numero osservazioni")

histPlot = count.byClass %>% ggplot(aes(x = `Classe di pianta`, y = `Numero osservazioni`)) + geom_col()
histPlot
```


Summary of the data
```{r}
summary(data)
```

Treat Class column as categories
```{r}
data$Class = as.factor(data$Class) # otherwise randomForest assumes regression
# mapvalues(data$Class, from = c(...), to = c(...)) to rename factor column names
```

Import random forest and tree
```{r}
require(randomForest)
require(tree)
```

Learn tree - based models
```{r}
modelRF = randomForest(Class ~ ., data, importance = T) # specimen number not relevant for classification
modelTB = randomForest(Class ~ ., data, mtry = ncol(data) - 1, importance = T) # tree bagging, same as random forest, but m = considering all variables - Class
modelST = tree(Class ~ ., data)
```

Random forest errors 
```{r}
modelRF$confusion
head(modelRF$err.rate)
```

Tree bagging errors 
```{r}
modelTB$confusion
head(modelTB$err.rate)
```

Random forest and tree bagging confusion matrix and OOB, from lab3.Rmd
```{r}
confusion.matrix.RF = modelRF$confusion[,-ncol(modelRF$confusion)]
1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)

confusion.matrix.TB = modelTB$confusion[,-ncol(modelTB$confusion)]
1-sum(diag(confusion.matrix.TB))/sum(confusion.matrix.TB)
```

Random forest and tree bagging variable importance
```{r}
modelRF$importance
modelTB$importance
```
Plot variable importance RF
```{r}
`Importanza variabili Random Forest` <- modelRF
varimp.RF = varImpPlot(`Importanza variabili Random Forest`)
```

Plot variable importance TB
```{r}
`Importanza variabili Tree Bagging` <- modelTB
varImpPlot(`Importanza variabili Tree Bagging`)
```


From lab3, error varying B with 10-CV 
```{r}
cv.error.RF.B = c(1:100) %>% map_dfr(function(b) {c(B = b, cv.test.err = mean(cv.error(Class ~ ., randomForest, data, 10, ntree=b)))}) %>% ggplot(aes(x=B, y=cv.test.err)) + geom_line() + ylim(0, 1)
cv.error.RF.B
```
Study of B for Tree bagging
```{r}
oob.error.TB.B = c(1:1000) %>% map_dfr(function(b) {
   model = randomForest(Class ~ ., data, mtry = ncol(data) - 1, ntree = b)
   confusion.matrix = model$confusion[,-ncol(model$confusion)]
   oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
   c(B = `Valore parametro B`, `Errore` = oob.error)}) %>% ggplot(aes(x = `Valore parametro B`, y = `Errore`)) + geom_line() + ylim(0, 1) # errors between 0 and 1
 oob.error.TB.B
```

Study of B for random forest
```{r}
oob.error.RF.B = c(1:1000) %>% map_dfr(function(b) {
   model = randomForest(Class ~ ., data, ntree = b)
   confusion.matrix = model$confusion[,-ncol(model$confusion)]
   oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
   c(`Valore parametro B` = b, `Errore` = oob.error)}) %>% ggplot(aes(x = `Valore parametro B`, y = `Errore`)) + geom_line() + ylim(0, 1)
 oob.error.RF.B
```

Adapted from lab3, error varying B using OOB instead of CV (RandomForest and TreeBagging)
```{r}
oob.error.B = c(1:2000) %>% map_dfr(function(b) {
  if (b <= 1000)
  {
      model.RF = randomForest(Class ~ ., data, ntree = b)
      confusion.matrix.RF = model.RF$confusion[,-ncol(model.RF$confusion)]
      oob.error.RF = 1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)
      c(`Valore parametro B` = b, Modello = "Random Forest", `Errore` = oob.error.RF)
  }
  else
  {
    b = b - 1000
    model.TB = randomForest(Class ~ ., data, ntree = b, mtry = ncol(data) - 1)
    confusion.matrix.TB = model.TB$confusion[,-ncol(model.TB$confusion)]
    oob.error.TB = 1-sum(diag(confusion.matrix.TB))/sum(confusion.matrix.TB)
    c(`Valore parametro B` = b, Modello = "Tree Bagging", `Errore` = oob.error.TB)
  }
})
oob.error.B$`Valore parametro B` = as.numeric(oob.error.B$`Valore parametro B`)
oob.error.B$Errore = as.double(oob.error.B$Errore)

 oob.error.B.plot = ggplot(oob.error.B, (aes(x = `Valore parametro B`, y = `Errore`, color = Modello))) + geom_line()
```

Study of m for random forest (with OOB error)
```{r}
oob.error.RF.m = c(1:15) %>% map_dfr(function(m) {
  model = randomForest(Class ~ ., data, mtry = m)
  confusion.matrix = model$confusion[,-ncol(model$confusion)]
  oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  c(`Valore parametro m` = m, `Errore` = oob.error)}) %>% ggplot(aes(x = `Valore parametro m`, y = `Errore`)) + geom_line() + ylim(0, 1)
oob.error.RF.m
```

Study m and B at the same time
```{r}
oob.error.B.m = seq(1, 100, 1) %>% map_dfr(function(b) {
  seq(1, 10, 1) %>% map_dfr(function(m) {
    model = randomForest(Class ~ ., data, ntree = b, mtry = m)
    confusion.matrix = model$confusion[,-ncol(model$confusion)]
    oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
    c(B = b, m = m, err = oob.error)
  })
})
```

And plot 3D function
```{r}
library(akima)
im = with(oob.error.B.m, interp(B, m, err))
with(im, image(unique(oob.error.B.m$B), unique(oob.error.B.m$m), matrix(oob.error.B.m$err, length(unique(oob.error.B.m$B)), length(unique(oob.error.B.m$m))), xlab = "Valore parametro B", ylab = "Valore parametro m"))

library(rgl)
plot3d(x = oob.error.B.m$B, y = oob.error.B.m$m, z = oob.error.B.m$err)
lines3d(x = oob.error.B.m$B, y = oob.error.B.m$m, z = oob.error.B.m$err)

library(scatterplot3d)
scatterplot3d(x = oob.error.B.m$B, y = oob.error.B.m$m, z = oob.error.B.m$err, xlab = "Valore parametro B", ylab = "Valore parametro m", zlab = "Errore OOB")
```


LOOCV errors for multiple k_min parameters
```{r}
cv.error.ST.kmin = c(1:50) %>% map_dfr(function(k_min) {c(`Valore parametro k_min` = k_min, `Errore` = mean(cv.error(Class ~ ., tree, data, nrow(data), mindev = 0, minsize = k_min)))}) %>% ggplot(aes(x=`Valore parametro k_min`, y=`Errore`)) + geom_line() + ylim(0, 1)
cv.error.ST.kmin
```

Optimal values for tree based methods
```{r}
optimal.RF.B = which(oob.error.RF.B$data$Errore == min(oob.error.RF.B$data$Errore)) # already ordered, use index instead of value
optimal.RF.m = which(oob.error.RF.m$data$Errore == min(oob.error.RF.m$data$Errore))
optimal.TB.B = which(oob.error.TB.B$data$Errore == min(oob.error.TB.B$data$Errore))
optimal.ST.kmin = which(cv.error.ST.kmin$data$Errore == min(cv.error.ST.kmin$data$Errore))
optimal.RF.B.m = oob.error.B.m[which(oob.error.B.m$err == min(oob.error.B.m$err)),]
```
   
SVM model
```{r}
require(e1071)
```

build SVM model
Default kernel: radial
```{r}
modelSVM = svm(Class ~ ., data)
```

CV errors for multiple parameters (from lab4)
```{r}
cv.error.SVM.C = expand_grid(kernel=c("linear","polynomial","radial","sigmoid"), `Valore parametro C`=exp(seq(-6,8,0.5))) %>% rowwise() %>% mutate(`Errore` = mean(cv.error(Class~., svm, data, 10, kernel=kernel, cost=`Valore parametro C`, degree=2)))
cv.error.SVM.C %>% ggplot(aes(x=`Valore parametro C`,y=`Errore`,color=kernel)) + scale_x_log10() + geom_line() + geom_point()
cv.error.SVM.C
```

Optimal values for SVM
```{r}
optimal.SVM.C = exp(-6 + (which(cv.error.SVM.C$Errore == min(cv.error.SVM.C$Errore)) - 1) * 0.5) # index times forward in sequence seq(-6,8,0.5)
optimal.SVM.kernel.table = cv.error.SVM.C[which(cv.error.SVM.C$Errore == min(cv.error.SVM.C$Errore)),1]
optimal.SVM.kernel = string(optimal.SVM.kernel.table$kernel)
```

LOOCV errors with detected optimal values

LOOCV tree
```{r}
mean(cv.error(Class ~ ., tree, data, nrow(data), minsize = optimal.ST.kmin))
```
LOOCV tree bagging
```{r}
mean(cv.error(Class ~ ., randomForest, data, nrow(data), mtry = ncol(data) - 1, B = optimal.TB.B)) # or could leave default

```

LOOCV RandomForest
```{r}
mean(cv.error(Class ~ ., randomForest, data, nrow(data), B = optimal.RF.B, mtry = optimal.RF.m))
```

LOOCV SVM
```{r}
mean(cv.error(Class ~ ., svm, data, nrow(data), kernel = optimal.SVM.kernel, cost = optimal.SVM.C))
```

LOOCV Naive Bayes 
```{r}
mean(cv.error(Class ~ ., naiveBayes, data, nrow(data)))
```



Random classifier
```{r}
error = 0
  for(i in c(1:nrow(data))){
  x = sample(data[,1] , size =1)
    if(x!=data[i,1]){
      error = error + 1;
    }  
  }
error_rate = error/nrow(data);
error_rate
```

kNN classification 

```{r}
trControl <- trainControl(method  = "cv",
                          number  = 5)
fit <- train(Species ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = data)
```


```{r}


indexes = sample(nrow(data))
  errs = c(1:nrow(data)) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/nrow(data)*(i-1)+1):(nrow(data)/nrow(data)*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = knn(data[-indexes.test,],indexes.test, data[indexes.test,"SpecimenNumber"], k=13)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(Class ~ .))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:nrow(data)))
  errs


