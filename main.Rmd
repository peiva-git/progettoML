---
title: "Progetto ML"
author: "Manuel Kosovel, Ivan Pelizon"
date: "8/12/2020"
output: html_document
---

Packages needed

```{r}
require(tidyverse)
require(rlang)
```


Useful imported functions
```{r}
# return vector of errors for each fold
# can specify formula, learning technique, data and number of folds (k)
cv.error = function(formula, learner, data, k, ...) {
  indexes = sample(nrow(data))
  errs = c(1:k) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/k*(i-1)+1):(nrow(data)/k*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = learner(formula, data[-indexes.test,], ...)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(formula))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:k))
  errs
}
```

User defined functions
```{r}
data["SpecimenNumber"]
```

First we import the csv
```{r}
data = read.csv("C:\\Users\\aleks\\Documents\\drive\\uni\\Progetti\\ML\\progetti 2019_20\\leaf\\leaf.csv")
#data = read.csv("/home/peiva/Documents/Sync documents/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
#data = read.csv("/home/peiva/Documents/Sync documents/Ivan/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
```

Change column names
```{r}
colnames(data) <- c("Class","SpecimenNumber","Eccentricity","AspectRatio","Elongation",
                    "Solidity","StochasticConvexity","IsoperimetricFactor",
                    "MaximalIndentationDepth","Lobedness","AverageIntensity" , 
                    "AverageContrast","Smoothness","ThirdMoment","Uniformity","Entropy")
# remove specimen count column
# data = data[,-2]
# SpecimenCount value reduces OOB error on RF by 3%, keep it
```

Histogram, available observations count
```{r}
count.byClass = data %>% group_by(Class) %>% count(Class)
colnames(count.byClass) = c("SpecimenClass", "count")

count.byClass %>% ggplot(aes(x = SpecimenClass, y = count)) + geom_col()
```


Summary of the data
```{r}
summary(data)
```

Treat Class column as categories
```{r}
data$Class = as.factor(data$Class) # otherwise randomForest assumes regression
# mapvalues(data$Class, from = c(...), to = c(...)) to rename factor column names
```

Import random forest
```{r}
require(randomForest)
require(tree)
```

Learn tree - based models
```{r}
modelRF = randomForest(Class ~ ., data, importance = T) # specimen number not relevant for classification
modelTB = randomForest(Class ~ ., data, mtry = ncol(data) - 1, importance = T) # tree bagging, same as random forest, but m = considering all variables - Class
modelST = tree(Class ~ ., data)
```

Random forest errors
```{r}
modelRF$confusion
head(modelRF$err.rate)
```

Tree bagging errors
```{r}
modelTB$confusion
head(modelTB$err.rate)
```

Random forest and tree bagging confusion matrix and OOB, from lab3.Rmd
```{r}
confusion.matrix.RF = modelRF$confusion[,-ncol(modelRF$confusion)]
1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)

confusion.matrix.TB = modelTB$confusion[,-ncol(modelTB$confusion)]
1-sum(diag(confusion.matrix.TB))/sum(confusion.matrix.TB)
```

Random forest and tree bagging variable importance
```{r}
modelRF$importance
modelTB$importance
```

From lab3, error varying B with 10-CV
```{r}
cv.error.RF.B = c(1:100) %>% map_dfr(function(b) {c(B = b, cv.test.err = mean(cv.error(Class ~ ., randomForest, data, 10, ntree=b)))}) %>% ggplot(aes(x=B, y=cv.test.err)) + geom_line() + ylim(0, 1)
cv.error.RF.B
```

Adapted from lab3, error varying B using OOB instead of CV
```{r}
oob.error.RF.B = c(1:100) %>% map_dfr(function(b) {
  model = randomForest(Class ~ ., data, ntree = b)
  confusion.matrix = model$confusion[,-ncol(model$confusion)]
  oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  c(B = b, oob.test.error = oob.error)}) %>% ggplot(aes(x = B, y = oob.test.error)) + geom_line() + ylim(0, 1)
oob.error.RF.B
```

LOOCV errors for multiple k_min parameters
```{r}
cv.error.ST.kmin = c(1:50) %>% map_dfr(function(k_min) {c(k = k_min, cv.test.err = mean(cv.error(Class ~ ., tree, data, nrow(data), mindev = 0, minsize = k_min)))}) %>% ggplot(aes(x=k, y=cv.test.err)) + geom_line() + ylim(0, 1)
cv.error.ST.kmin
```
   
Learn SVM model
```{r}
require(e1071)
```

build SVM model
Default kernel: radial
```{r}
modelSVM = svm(Class ~ ., data)
```

CV errors for multiple parameters (from lab4)
```{r}
results = expand_grid(kernel=c("linear","polynomial","radial","sigmoid"), cost=exp(seq(-6,3,0.1))) %>% rowwise() %>% mutate(error = mean(cv.error(Class~., svm, data, 10, kernel=kernel, cost=cost, degree=2)))
results %>% ggplot(aes(x=cost,y=error,color=kernel)) + geom_line() + geom_point()
results
```

LOOCV errors

LOOCV error tree
```{r}
err = cv.error(Class ~ ., tree, data, nrow(data))
mean(err)
```
LOOCV tree bagging
```{r}
err = cv.error(Class ~ ., randomForest, data, nrow(data), mtry = ncol(data) - 1, importance = T)
mean(err)
```

LOOCV RandomForest
```{r}
err = cv.error(Class ~ ., randomForest, data, nrow(data), importance = T)
mean(err)
```

LOOCV SVM
```{r}
err = cv.error(Class ~ ., svm, data, nrow(data))
mean(err)
```

LOOCV Naive Bayes 
```{r}
err = cv.error(Class ~ ., naiveBayes, data, nrow(data))
mean(err)
```


kNN classification

```{r}
require(class)
```


```{r}


indexes = sample(nrow(data))
  errs = c(1:nrow(data)) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/nrow(data)*(i-1)+1):(nrow(data)/nrow(data)*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = knn(data[-indexes.test,],indexes.test, data["SpecimenNumber"], k=13)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(Class ~ .))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:nrow(data)))
  errs
```
```






