---
title: "Progetto ML"
author: "Manuel Kosovel, Ivan Pelizon"
date: "8/12/2020"
output: html_document
---

Packages needed

```{r}
require(tidyverse)
require(rlang)
```


Useful imported functions
```{r}
# return vector of errors for each fold
# can specify formula, learning technique, data and number of folds (k)
cv.error = function(formula, learner, data, k, ...) {
  indexes = sample(nrow(data))
  errs = c(1:k) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/k*(i-1)+1):(nrow(data)/k*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = learner(formula, data[-indexes.test,], ...)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(formula))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:k))
  errs
}
```

User defined functions
```{r}

```

First we import the csv
```{r}
#data = read.csv("C:\\Users\\aleks\\Documents\\drive\\uni\\Progetti\\ML\\progetti 2019_20\\leaf\\leaf.csv")
data = read.csv("/home/peiva/Documents/Sync documents/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
```

Change column names
```{r}
colnames(data) <- c("Class","SpecimenNumber","Eccentricity","AspectRatio","Elongation",
                    "Solidity","StochasticConvexity","IsoperimetricFactor",
                    "MaximalIndentationDepth","Lobedness","AverageIntensity" , 
                    "AverageContrast","Smoothness","ThirdMoment","Uniformity","Entropy")
# remove specimen count column
data = data[,-2]
```


Histogram (to modify)
```{r}

we <- data$Class
we <- gsub(",", "", we)   # remove comma
we <- as.numeric(we)      # turn into numbers
hist(we)

```


Summary of the data
```{r}
summary(data)
```

Treat Class column as categories
```{r}
data$Class = as.factor(data$Class) # otherwise randomForest assumes regression
# mapvalues(data$Class, from = c(...), to = c(...)) to rename factor column names
```

Import random forest
```{r}
require(randomForest)
```

Learn tree - based models
```{r}
modelRF = randomForest(Class ~ ., data, importance = T) # specimen number not relevant for classification
modelTB = randomForest(Class ~ ., data, mtry = ncol(data) - 1, importance = T) # tree bagging, same as random forest, but m = considering all variables - Class
```

Random forest errors
```{r}
modelRF$confusion
head(modelRF$err.rate)
```
Tree bagging errors
```{r}
modelTB$confusion
head(modelTB$err.rate)
```

Random forest confusion matrix and OOB, from lab3.Rmd
```{r}
confusion.matrix.RF = modelRF$confusion[,-ncol(modelRF$confusion)]
1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)
```

Tree bagging confusion matrix and OOB
```{r}
confusion.matrix.TB = modelTB$confusion[,-ncol(modelTB$confusion)]
1-sum(diag(confusion.matrix.TB))/sum(confusion.matrix.TB)
```

Considering variable importance
```{r}
modelRF$importance
modelTB$importance
```

Error rate with 5-CV
```{r}
cv.error(Class ~ ., randomForest, data, nrow(data))
```

Error rates with different k-CV ("long" computation time) - NOT USEFUL
```{r}
k_value = seq(2, 10, 1)
error = k_value %>% map_dbl(function(r) {mean(cv.error(Class~.-SpecimenNumber, randomForest, data, r))})
tibble(k_value = k_value, error = error) %>% ggplot(aes(x=k_value, y=error)) + geom_line()
```

From lab3
```{r}
cv.error.b = c(1:100) %>% map_dfr(function(b) {c(B = b, cv.test.err = mean(cv.error(Class ~ ., randomForest, data, 10, ntree=b)))}) %>% ggplot(aes(x=B, y=cv.test.err)) + geom_line() + ylim(0, 1)
```

Adapted from lab3, using oob instead of CV
```{r}
oob.error.b = c(1:100) %>% map_dfr(function(b) {
  model = randomForest(Class ~ ., data, ntree = b)
  confusion.matrix = model$confusion[,-ncol(model$confusion)]
  oob.error = 1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  c(B = b, oob.test.error = oob.error)}) %>% ggplot(aes(x = B, y = oob.test.error)) + geom_line() + ylim(0, 1)
```

TODO: estimate learning techniques with CV (auto parameter or with predefined parameter)
      add SVM and single tree learning techniques (and also boosting variant?)
      
      
SINGLE TREE

```{r}
require(tree)
```

build tree
```{r}
modelST = tree(Class ~ ., data)
```
```{r}
plot(singleTree)
text(singleTree)
```

```{r}
cv.modelST = cv.error(Class ~ ., tree , data, nrow(data))
mean(cv_tree)
```

10-CV errors for multiple single tree parameters
```{r}
cv.error.kmin.ST = c(1:50) %>% map_dfr(function(k_min) {c(k = k_min, cv.test.err = mean(cv.error(Class ~ ., tree, data, nrow(data), mindev = 0, minsize = k_min)))}) %>% ggplot(aes(x=k, y=cv.test.err)) + geom_line() + ylim(0, 1)
```


```{r}
k_value = seq(2, 300 , 10)
error = k_value %>% map_dbl(function(r) {mean(cv.error(Class ~ ., tree , data, 5))})
tibble(k_value = k_value, error = error) %>% ggplot(aes(x=k_value, y=error)) + geom_line() + ylim(0,1)
```


      
SVM 

```{r}
require(e1071)
```

build svm model
Default kernel: radial
```{r}
modelSVM = svm(Class ~ . - SpecimenNumber, data)
```


We can use the cv.error function 

```{r}
cv.error(formula = Class ~ . - SpecimenNumber, svm, data, 5, kernel="polynomial", degree=2)
```


```{r}
k_value = seq(2, 10, 1)
error = k_value %>% map_dbl(function(r) {mean(cv.error(formula = Class ~ . - SpecimenNumber, svm, data, 5, kernel="polynomial", degree=2))})
tibble(k_value = k_value, error = error) %>% ggplot(aes(x=k_value, y=error)) + geom_line()
```




      
      
