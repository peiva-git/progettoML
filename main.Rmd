---
title: "Progetto ML"
author: "Manuel Kosovel, Ivan Pelizon"
date: "8/12/2020"
output: html_document
---

Useful imported functions
```{r}
# return vector of errors for each fold
# can specify formula, learning technique, data and number of folds (k)
cv.error = function(formula, learner, data, k, ...) {
  indexes = sample(nrow(data))
  errs = c(1:k) %>% map_dbl(function(i) {
    indexes.test = indexes[c((nrow(data)/k*(i-1)+1):(nrow(data)/k*i))] # 1 -> n/k, (n/k) + 1 -> 2*(n/k), ...
    m = learner(formula, data[-indexes.test,], ...)
    predicted.y = predict(m, data[indexes.test,], type = "class")
    actual.y = data[indexes.test, as.character(f_lhs(formula))]
    confusion.matrix = table(actual.y, predicted.y)
    1-sum(diag(confusion.matrix))/sum(confusion.matrix)
  })
  names(errs) = paste0("fold", c(1:k))
  errs
}
```

First we import the csv
```{r}
data = read.csv("/home/peiva/Documents/Sync documents/units/introduction to machine learning and evolutionary robotics/progetti_2019_20/leaf identification/leaf.csv")
```

Change column names
```{r}
colnames(data) <- c("Class","SpecimenNumber","Eccentricity","AspectRatio","Elongation",
                    "Solidity","StochasticConvexity","IsoperimetricFactor",
                    "MaximalIndentationDepth","Lobedness","AverageIntensity" , 
                    "AverageContrast","Smoothness","ThirdMoment","Uniformity","Entropy")
```

Summary of the data
```{r}
summary(data)
```
```{r}
data$Class = as.factor(data$Class) # otherwise randomForest assumes regression
# mapvalues(data$Class, from = c(...), to = c(...)) to rename factor column names
```

Import random forest
```{r}
require(randomForest)
```

Learn tree - based models
```{r}
modelRF = randomForest(Class ~ . - SpecimenNumber, data) # specimen number not relevant for classification
modelTB = randomForest(Class ~ . - SpecimenNumber, data, mtry = ncol(data) - 2) # tree bagging, same as random forest, but considering all variables - (SpecimenName + Class)
```

Random forest errors
```{r}
modelRF$confusion
head(modelRF$err.rate)
```
Tree bagging errors
```{r}
modelTB$confusion
head(modelTB$err.rate)
```

Random forest confusion matrix and OOB, from lab3.Rmd
```{r}
confusion.matrix.RF = modelRF$confusion[,-ncol(modelRF$confusion)]
1-sum(diag(confusion.matrix.RF))/sum(confusion.matrix.RF)
```

Tree bagging confusion matrix and OOB
```{r}
confusion.matrix.TB = modelTB$confusion[,-ncol(modelTB$confusion)]
1-sum(diag(confusion.matrix.TB))/sum(confusion.matrix.TB)
```

Considering variable importance
```{r}
modelRF$importance
modelTB$importance
```

Error rate with 5-CV
```{r}
cv.error(Class ~ . - SpecimenNumber, randomForest, data, 5)
```
Error rates with different k-CV ("long" computation time)
```{r}
k_value = seq(2, 10, 1)
error = k_value %>% map_dbl(function(r) {mean(cv.error(Class~.-SpecimenNumber, randomForest, data, r))})
tibble(k_value = k_value, error = error) %>% ggplot(aes(x=k_value, y=error)) + geom_line()
```

TODO: estimate learning techniques with CV (auto parameter or with predefined parameter)
      add SVM and single tree learning techniques (and also boosting variant?)
